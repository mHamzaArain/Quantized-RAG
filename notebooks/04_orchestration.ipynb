{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"/home/kumail/Quantized-RAG\")\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumail/Quantized-RAG/envs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class Vector:\n",
    "    def __init__(self, model_name, index_file, documents_file) :\n",
    "        self.embed_tokenizer, self.embed_model=self.load_model(model_name)\n",
    "        self.faiss_index = self.load_index(index_file)\n",
    "        self.docs = self.load_embedded_documents(documents_file)\n",
    "\n",
    "        print(\"Vector\")\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        return tokenizer, model\n",
    "    \n",
    "    def load_index(self, index_file):\n",
    "        return faiss.read_index(index_file)\n",
    "    \n",
    "    def load_embedded_documents(self, documents_file):\n",
    "        with open(documents_file, 'rb') as f:\n",
    "            documents = pickle.load(f)\n",
    "            return documents\n",
    "        \n",
    "    def compute_embeddings(self, texts):\n",
    "        inputs = self.embed_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            model_output = self.embed_model(**inputs)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1)  # mean pooling\n",
    "        return embeddings.numpy()\n",
    "\n",
    "    def cosine_search(self, query_embedding, k):\n",
    "        distances, indices = self.faiss_index.search(query_embedding, k)\n",
    "        return indices\n",
    "    \n",
    "    def search(self, text, k):\n",
    "        query_embedding = self.compute_embeddings([text])\n",
    "        retrived_indexes = self.cosine_search(query_embedding, k)\n",
    "        return [self.docs[i] for i in retrived_indexes[0]]\n",
    "\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "documents_file = \"data/documents.pkl\"\n",
    "index_file = \"data/faiss_index.bin\"\n",
    "\n",
    "vec = Vector(model_name=model_name,\n",
    "                index_file=index_file,\n",
    "                documents_file=documents_file\n",
    "            )\n",
    "\n",
    "ask = \"What is the revenue of apple\"\n",
    "data = vec.search(ask, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "class PromotEngineer:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "        print(\"PE\")\n",
    "\n",
    "\n",
    "    def prompt(self, essentials):\n",
    "        prompt_adjuster = PromptTemplate(\n",
    "            input_variables=[\"data\", \"question\"], template=self.prompt_template\n",
    "        )\n",
    "\n",
    "        return prompt_adjuster.format(**essentials)\n",
    "\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Only few words answer\n",
    "{data}\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_setter = PromotEngineer(prompt_template=PROMPT_TEMPLATE)\n",
    "\n",
    "data = {\n",
    "    \"data\": data,\n",
    "    \"question\": ask\n",
    "}\n",
    "\n",
    "prompted_ask =  prompt_setter.prompt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Only few words answer\\n[\"Apple Inc.\\'s total revenue in 2022 was $394.33 billion.\", \"Apple\\'s Revenue in the last fiscal year was 385.71 billion USD.\"]\\nWhat is the revenue of apple\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompted_ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumail/Quantized-RAG/envs/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, llm_url, llm_model_name):\n",
    "        self.llm_url = llm_url\n",
    "        self.llm_model_name = llm_model_name\n",
    "        print(\"LLM\")\n",
    "\n",
    "\n",
    "    def ask_to_llm(self, ask):\n",
    "        ollama = Ollama(base_url=self.llm_url, model=self.llm_model_name)\n",
    "        return ollama(ask)  \n",
    "\n",
    "\n",
    "url = \"http://localhost:11434\"\n",
    "model_name = \"phi3\"\n",
    "\n",
    "llm = LLM(llm_url=url, llm_model_name=model_name)\n",
    "\n",
    "# ask = \"hi\"\n",
    "response  = llm.ask_to_llm(prompted_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$394.33 billion.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector\n",
      "PE\n",
      "LLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'$394.33 billion\\n\\n$385.71 billion'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Orchestration(Vector, PromotEngineer, LLM):\n",
    "    def __init__(self, model_name, index_file, documents_file, prompt_template, llm_url, llm_model_name):\n",
    "        Vector.__init__(self, model_name=model_name, index_file=index_file, documents_file=documents_file)\n",
    "        PromotEngineer.__init__(self, prompt_template=prompt_template)\n",
    "        LLM.__init__(self, llm_url=llm_url, llm_model_name=llm_model_name)\n",
    "\n",
    "    def run(self, ask):\n",
    "        # Perform operations using the orchestration object\n",
    "        # Data Retrieval from Vector DB\n",
    "        data = self.search(ask, 2)\n",
    "        \n",
    "        data_dict = {\"data\": data, \"question\": ask}\n",
    "        \n",
    "        # Prompt Setter\n",
    "        prompted_ask = self.prompt(data_dict)\n",
    "        \n",
    "        # Response to user\n",
    "        response = orchestration.ask_to_llm(prompted_ask)\n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "documents_file = \"data/documents.pkl\"\n",
    "index_file = \"data/faiss_index.bin\"\n",
    "ask = \"What is the revenue of apple\"\n",
    "PROMPT_TEMPLATE = \"\"\"Only few words answer\n",
    "{data}\n",
    "{question}\n",
    "\"\"\"\n",
    "url = \"http://localhost:11434\"\n",
    "llm_model_name = \"phi3\"\n",
    "\n",
    "orchestration = Orchestration(\n",
    "    model_name=model_name,\n",
    "    index_file=index_file,\n",
    "    documents_file=documents_file,\n",
    "    prompt_template=PROMPT_TEMPLATE,\n",
    "    llm_url=url,\n",
    "    llm_model_name=llm_model_name\n",
    ")\n",
    "\n",
    "orchestration.run(ask=ask)\n",
    "\n",
    "# # Perform operations using the orchestration object\n",
    "# data = orchestration.search(ask, 2)\n",
    "# data_dict = {\"data\": data, \"question\": ask}\n",
    "# prompted_ask = orchestration.prompt(data_dict)\n",
    "# response = orchestration.ask_to_llm(prompted_ask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$394.33 billion'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
